{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1315f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f6d57",
   "metadata": {},
   "source": [
    "## STEP 2  - STOPWORD ko define karege"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba40ee16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4fad842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stop_words)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21495169",
   "metadata": {},
   "source": [
    "## STEP 3 - Data Template - INput Data (Random data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ba812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job Description ke parts\n",
    "job_description_parts = {\n",
    "    \"role\": [\n",
    "        \"Senior Data Scientist\",\n",
    "        \"Machine Learning Engineer\",\n",
    "        \"Data Analyst\",\n",
    "        \"NLP Engineer\",\n",
    "        \"ML Operations Engineer\"\n",
    "    ],\n",
    "\n",
    "    \"responsibilities\": [\n",
    "        [\n",
    "            \"develop machine learning models for predictive analytics\",\n",
    "            \"analyze large datasets to drive business insights\",\n",
    "            \"implement data pipelines using Python and SQL\",\n",
    "            \"collaborate with cross-functional teams on data initiatives\"\n",
    "        ],\n",
    "        [\n",
    "            \"design and build deep learning systems for production\",\n",
    "            \"optimize neural network architectures for inference\",\n",
    "            \"conduct research on state-of-the-art NLP techniques\",\n",
    "            \"deploy models using Docker and Kubernetes\"\n",
    "        ],\n",
    "        [\n",
    "            \"perform exploratory data analysis on business metrics\",\n",
    "            \"create dashboards for stakeholder reporting\",\n",
    "            \"develop statistical models for customer behavior prediction\",\n",
    "            \"maintain data quality and governance standards\"\n",
    "        ],\n",
    "        [\n",
    "            \"build NLP pipelines for text classification and entity recognition\",\n",
    "            \"fine-tune transformer models for specific use cases\",\n",
    "            \"implement named entity recognition systems\",\n",
    "            \"develop chatbots using sequence-to-sequence models\"\n",
    "        ]\n",
    "    ],\n",
    "\n",
    "    \"requirements\": [\n",
    "        [\n",
    "            \"5+ years experience in data science and machine learning\",\n",
    "            \"proficiency in Python, R, and SQL\",\n",
    "            \"strong understanding of statistical concepts\",\n",
    "            \"experience with scikit-learn, TensorFlow, or PyTorch\"\n",
    "        ],\n",
    "        [\n",
    "            \"masters degree in computer science or related field\",\n",
    "            \"3+ years building production ML systems\",\n",
    "            \"expertise in deep learning frameworks\",\n",
    "            \"familiarity with cloud platforms like AWS or GCP\"\n",
    "        ],\n",
    "        [\n",
    "            \"proficiency in Python and SQL\",\n",
    "            \"knowledge of Tableau or PowerBI for visualization\",\n",
    "            \"understanding of business metrics and KPIs\",\n",
    "            \"experience with data warehousing tools\"\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Resume ke parts\n",
    "resume_parts = {\n",
    "    \"experience\": [\n",
    "        [\n",
    "            \"developed machine learning models using TensorFlow and PyTorch\",\n",
    "            \"optimized data pipelines reducing processing time by 60%\",\n",
    "            \"deployed Python Flask applications on AWS EC2 instances\",\n",
    "            \"implemented scikit-learn models achieving 95% accuracy\"\n",
    "        ],\n",
    "        [\n",
    "            \"built neural networks for image classification tasks\",\n",
    "            \"performed feature engineering on large-scale datasets\",\n",
    "            \"collaborated with product teams on model implementation\",\n",
    "            \"reduced model inference latency from 500ms to 50ms\"\n",
    "        ],\n",
    "        [\n",
    "            \"analyzed customer behavior using statistical methods\",\n",
    "            \"created dashboards in Tableau for executive reporting\",\n",
    "            \"performed SQL queries on petabyte-scale databases\",\n",
    "            \"improved data quality by implementing validation frameworks\"\n",
    "        ],\n",
    "        [\n",
    "            \"implemented NLP models for text classification\",\n",
    "            \"built transformers for named entity recognition\",\n",
    "            \"developed chatbots using sequence models\",\n",
    "            \"fine-tuned BERT for domain-specific applications\"\n",
    "        ]\n",
    "    ],\n",
    "\n",
    "    \"skills\": [\n",
    "        [\"Python\", \"Machine Learning\", \"TensorFlow\", \"SQL\", \"Statistics\"],\n",
    "        [\"Deep Learning\", \"PyTorch\", \"NLP\", \"Transformers\", \"BERT\"],\n",
    "        [\"Data Analysis\", \"Python\", \"SQL\", \"Tableau\", \"Excel\"],\n",
    "        [\"NLP\", \"Python\", \"Hugging Face\", \"Transformers\", \"Information Extraction\"]\n",
    "    ],\n",
    "\n",
    "    \"education\": [\n",
    "        \"B.Tech Computer Science from IIT\",\n",
    "        \"M.S. Data Science from top university\",\n",
    "        \"B.E. Electronics and Communication\",\n",
    "        \"M.Tech Machine Learning from premiere institute\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402d6c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Job role types: 5\n",
      "   - Resume types: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"   - Job role types: {len(job_description_parts['role'])}\")\n",
    "print(f\"   - Resume types: {len(resume_parts['experience'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46ae77",
   "metadata": {},
   "source": [
    "#### Gemerate the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8215bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_job_desc(n_jobs=3):\n",
    "  \n",
    "\n",
    "    jobs = []\n",
    "\n",
    "    for i in range(n_jobs):\n",
    "        # Cycle through roles (first 3 get different roles)\n",
    "        role_idx = i % len(job_description_parts['role'])\n",
    "        role = job_description_parts['role'][role_idx]\n",
    "\n",
    "        # Pick corresponding responsibilities and requirements\n",
    "        responsibilities = job_description_parts['responsibilities'][role_idx]\n",
    "        requirements = job_description_parts['requirements'][role_idx]\n",
    "\n",
    "        # Create full JD text\n",
    "        description = f\"\"\"\n",
    "        JOB DESCRIPTION: {role}\n",
    "\n",
    "        Responsibilities:\n",
    "        {'. '.join(responsibilities)}.\n",
    "\n",
    "        Requirements:\n",
    "        {'. '.join(requirements)}.\n",
    "\n",
    "        We are looking for talented professionals to join our team.\n",
    "        \"\"\"\n",
    "\n",
    "        jobs.append({\n",
    "            'job_id': f'JD_{i+1}',\n",
    "            'title': role,\n",
    "            'description': description\n",
    "        })\n",
    "\n",
    "    return jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e772589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_resumes(n_resumes=10):\n",
    "   \n",
    "\n",
    "    resumes = []\n",
    "\n",
    "    for i in range(n_resumes):\n",
    "        # Cycle through experience types\n",
    "        exp_idx = i % len(resume_parts['experience'])\n",
    "        experience = resume_parts['experience'][exp_idx]\n",
    "        skills = resume_parts['skills'][exp_idx]\n",
    "        education = random.choice(resume_parts['education'])\n",
    "        years_exp = random.randint(1, 10)\n",
    "\n",
    "        candidate_name = f\"Candidate_{i+1}\"\n",
    "\n",
    "        resume_text = f\"\"\"\n",
    "        RESUME: {candidate_name}\n",
    "\n",
    "        Education:\n",
    "        {education}\n",
    "\n",
    "        Years of Experience: {years_exp} years\n",
    "\n",
    "        Professional Experience:\n",
    "        {'. '.join(experience)}.\n",
    "\n",
    "        Technical Skills:\n",
    "        {', '.join(skills)}\n",
    "\n",
    "        Contact: {candidate_name.lower()}@email.com | +91-9XXXXXXXXX\n",
    "        \"\"\"\n",
    "\n",
    "        resumes.append({\n",
    "            'resume_id': f'RES_{i+1}',\n",
    "            'candidate_name': candidate_name,\n",
    "            'years_experience': years_exp,\n",
    "            'resume_text': resume_text\n",
    "        })\n",
    "\n",
    "    return resumes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28cfb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = generate_job_desc(3)\n",
    "resumes = generate_resumes(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "479b68ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 12)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jobs), len(resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09caa5",
   "metadata": {},
   "source": [
    "## Step 4 - Text Preprocessing\n",
    "\n",
    "1. Lowercase\n",
    "2. Remove Special Character\n",
    "3. Remove extra spaces\n",
    "4. Tokenization\n",
    "5. Remove stopwords\n",
    "6. Filter Short Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37661320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    tokens = text.split()\n",
    "    \n",
    "    tokens = [\n",
    "        token for token in tokens if token not in stop_words and len(token)>1\n",
    "    ]\n",
    "    \n",
    "    cleanedText = ' '.join(tokens)\n",
    "    \n",
    "    return cleanedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4d9e5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'good programmer love teach code jfdlkj2lkj dkkjad flkj'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('I am a good programmer, and i love to teach and code ;;jfdlkj2lkj     dkkjad;flkj  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a44c77c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANING TEXT.....\n",
      "--------------------------------------------------------------------------------\n",
      "BEFORE CLEANING\n",
      "\n",
      "        JOB DESCRIPTION: Senior Data Scientist\n",
      "\n",
      "        Responsibilities:\n",
      "        develop machine l\n",
      "--------------------------------------------------------------------------------\n",
      "After CLEANING\n",
      "job description senior data scientist responsibilities develop machine learning models predictive an\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"CLEANING TEXT.....\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "cleaned_jobs = []\n",
    "for job in jobs:\n",
    "    cleaned_desc = clean_text(job['description'])\n",
    "    cleaned_jobs.append({\n",
    "        'job_id': job['job_id'],\n",
    "        'title': job['title'],\n",
    "        'original_text': job['description'],\n",
    "        'cleaned_text': cleaned_desc\n",
    "    })\n",
    "\n",
    "print(\"BEFORE CLEANING\")\n",
    "print(jobs[0]['description'][:100])\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"After CLEANING\")\n",
    "print(cleaned_jobs[0]['cleaned_text'][:100])\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "838488d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CLEANING\n",
      "\n",
      "        RESUME: Candidate_1\n",
      "\n",
      "        Education:\n",
      "        M.Tech Machine Learning from premiere insti\n",
      "--------------------------------------------------------------------------------\n",
      "After CLEANING\n",
      "resume candidate education tech machine learning premiere institute years experience 10 years profes\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cleaned_resumes = []\n",
    "for resume in resumes:\n",
    "    cleaned_res = clean_text(resume['resume_text'])\n",
    "    cleaned_resumes.append({\n",
    "        'resume_id': resume['resume_id'],\n",
    "        'candidate_name': resume['candidate_name'],\n",
    "        'years_experience': resume['years_experience'],\n",
    "        'original_text': resume['resume_text'],\n",
    "        'cleaned_text': cleaned_res\n",
    "    })\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"BEFORE CLEANING\")\n",
    "print(resumes[0]['resume_text'][:100])\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"After CLEANING\")\n",
    "print(cleaned_resumes[0]['cleaned_text'][:100])\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b84fc",
   "metadata": {},
   "source": [
    "## Vectorizationj - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "651fe140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total texts to vectorize: 15\n",
      "   - Jobs: 3\n",
      "   - Resumes: 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_cleaned_text = []\n",
    "text_sources = []\n",
    "\n",
    "\n",
    "for job in cleaned_jobs:\n",
    "    all_cleaned_text.append(job['cleaned_text'])\n",
    "    text_sources.append({'type': 'job', 'id': job['job_id'], 'title': job['title']})\n",
    "    \n",
    "\n",
    "for resume in cleaned_resumes:\n",
    "    all_cleaned_text.append(resume['cleaned_text'])\n",
    "    text_sources.append({'type': 'resume', 'id': resume['resume_id'], 'name':resume['candidate_name']})\n",
    "    \n",
    "print(f\"‚úÖ Total texts to vectorize: {len(all_cleaned_text)}\")\n",
    "print(f\"   - Jobs: {len(cleaned_jobs)}\")\n",
    "print(f\"   - Resumes: {len(cleaned_resumes)}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "76d618ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features = 5000,\n",
    "    ngram_range= (1,2),\n",
    "    min_df = 2,\n",
    "    max_df = 0.8, \n",
    "    lowercase=True,\n",
    "    stop_words = 'english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d28f7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(all_cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "25bd8dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vectorization complete!\n",
      "\n",
      "Matrix Details:\n",
      "   Shape: (15, 284)\n",
      "   ‚Üí 15 documents (rows)\n",
      "   ‚Üí 284 features (columns)\n",
      "   ‚Üí Sparsity: 72.54%\n",
      "      (High sparsity = memory efficient! Mostly zeros)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"‚úÖ Vectorization complete!\\n\")\n",
    "print(f\"Matrix Details:\")\n",
    "print(f\"   Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"   ‚Üí {tfidf_matrix.shape[0]} documents (rows)\")\n",
    "print(f\"   ‚Üí {tfidf_matrix.shape[1]} features (columns)\")\n",
    "print(f\"   ‚Üí Sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "print(f\"      (High sparsity = memory efficient! Mostly zeros)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1e7c6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Vocabulary size: 284\n",
      "   Sample terms: ['10', '500ms', '500ms 50ms', '50ms', '50ms technical', '60', '60 deployed', '91', '91 9xxxxxxxxx', '95', '95 accuracy', '9xxxxxxxxx', 'accuracy', 'accuracy technical', 'achieving']\n",
      "\n",
      "‚úÖ Vectors separated:\n",
      "   Job vectors: (3, 284)\n",
      "   Resume vectors: (12, 284)\n"
     ]
    }
   ],
   "source": [
    "# Show vocabulary sample\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"üìö Vocabulary size: {len(feature_names)}\")\n",
    "print(f\"   Sample terms: {list(feature_names[:15])}\\n\")\n",
    "\n",
    "# Separate job and resume vectors\n",
    "n_jobs = len(cleaned_jobs)\n",
    "n_resumes = len(cleaned_resumes)\n",
    "\n",
    "job_vectors = tfidf_matrix[:n_jobs]\n",
    "resume_vectors = tfidf_matrix[n_jobs:]\n",
    "\n",
    "print(f\"‚úÖ Vectors separated:\")\n",
    "print(f\"   Job vectors: {job_vectors.shape}\")\n",
    "print(f\"   Resume vectors: {resume_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ac7f9",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ede127b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã JOB: Senior Data Scientist (ID: JD_1)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üèÜ Top 3 Matching Candidates:\n",
      "\n",
      "1. Candidate_1 (10 yrs)\n",
      "   Score: 0.3039 (30.4%)\n",
      "   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "\n",
      "2. Candidate_9 (9 yrs)\n",
      "   Score: 0.2801 (28.0%)\n",
      "   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "\n",
      "3. Candidate_5 (6 yrs)\n",
      "   Score: 0.2801 (28.0%)\n",
      "   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "\n",
      "\n",
      "üìã JOB: Machine Learning Engineer (ID: JD_2)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üèÜ Top 3 Matching Candidates:\n",
      "\n",
      "1. Candidate_6 (5 yrs)\n",
      "   Score: 0.1810 (18.1%)\n",
      "   ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "\n",
      "2. Candidate_2 (9 yrs)\n",
      "   Score: 0.1743 (17.4%)\n",
      "   ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "\n",
      "3. Candidate_1 (10 yrs)\n",
      "   Score: 0.1612 (16.1%)\n",
      "   ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "\n",
      "\n",
      "üìã JOB: Data Analyst (ID: JD_3)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üèÜ Top 3 Matching Candidates:\n",
      "\n",
      "1. Candidate_3 (9 yrs)\n",
      "   Score: 0.2698 (27.0%)\n",
      "   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "\n",
      "2. Candidate_11 (5 yrs)\n",
      "   Score: 0.2670 (26.7%)\n",
      "   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "\n",
      "3. Candidate_7 (3 yrs)\n",
      "   Score: 0.2656 (26.6%)\n",
      "   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for job_id, job in enumerate(cleaned_jobs):\n",
    "    print(f\"\\nüìã JOB: {job['title']} (ID: {job['job_id']})\")\n",
    "    print(\"-\" * 70)\n",
    "    # print(job_vectors[job_id])\n",
    "    job_vector = job_vectors[job_id]\n",
    "    \n",
    "    similarities = cosine_similarity(job_vector, resume_vectors)\n",
    "    # print(similarities)\n",
    "    \n",
    "    #flat to 1d array\n",
    "    flat_similarities = similarities.flatten()\n",
    "    \n",
    "    \n",
    "    #store the result\n",
    "    \n",
    "    for resume_idx, resume in enumerate(cleaned_resumes):\n",
    "        score = flat_similarities[resume_idx]\n",
    "        \n",
    "        all_results.append({\n",
    "            'job_id': job['job_id'],\n",
    "            'job_title': job['title'],\n",
    "            'resume_id': resume['resume_id'],\n",
    "            'candidate_name': resume['candidate_name'],\n",
    "            'years_experience': resume['years_experience'],\n",
    "            'similarity_score': score\n",
    "        })\n",
    "        \n",
    "    # Print top 3 matches for this job\n",
    "    top_k = 3\n",
    "    top_indices = np.argsort(flat_similarities)[::-1][:top_k]\n",
    "\n",
    "    print(f\"\\nüèÜ Top {top_k} Matching Candidates:\")\n",
    "    print()\n",
    "\n",
    "    for rank, resume_idx in enumerate(top_indices, 1):\n",
    "        score = flat_similarities[resume_idx]\n",
    "        candidate = cleaned_resumes[resume_idx]\n",
    "        score_percent = score * 100\n",
    "\n",
    "        # Visual bar\n",
    "        bar_length = int(score_percent / 5)\n",
    "        bar = \"‚ñà\" * bar_length + \"‚ñë\" * (20 - bar_length)\n",
    "\n",
    "        print(f\"{rank}. {candidate['candidate_name']} ({candidate['years_experience']} yrs)\")\n",
    "        print(f\"   Score: {score:.4f} ({score_percent:.1f}%)\")\n",
    "        print(f\"   {bar}\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a6d908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä COMPLETE MATCHING RESULTS\n",
      "====================================================================================================\n",
      "\n",
      "job_id                 job_title resume_id candidate_name  years_experience  similarity_score\n",
      "  JD_1     Senior Data Scientist     RES_1    Candidate_1                10          0.303868\n",
      "  JD_1     Senior Data Scientist     RES_5    Candidate_5                 6          0.280105\n",
      "  JD_1     Senior Data Scientist     RES_9    Candidate_9                 9          0.280105\n",
      "  JD_1     Senior Data Scientist     RES_7    Candidate_7                 3          0.165742\n",
      "  JD_1     Senior Data Scientist    RES_11   Candidate_11                 5          0.134090\n",
      "  JD_1     Senior Data Scientist     RES_3    Candidate_3                 9          0.126774\n",
      "  JD_1     Senior Data Scientist    RES_10   Candidate_10                 9          0.106250\n",
      "  JD_1     Senior Data Scientist     RES_2    Candidate_2                 9          0.101572\n",
      "  JD_1     Senior Data Scientist    RES_12   Candidate_12                 3          0.077467\n",
      "  JD_1     Senior Data Scientist     RES_6    Candidate_6                 5          0.069877\n",
      "  JD_1     Senior Data Scientist     RES_8    Candidate_8                 1          0.069570\n",
      "  JD_1     Senior Data Scientist     RES_4    Candidate_4                 2          0.036782\n",
      "  JD_2 Machine Learning Engineer     RES_6    Candidate_6                 5          0.181024\n",
      "  JD_2 Machine Learning Engineer     RES_2    Candidate_2                 9          0.174319\n",
      "  JD_2 Machine Learning Engineer     RES_1    Candidate_1                10          0.161209\n",
      "  JD_2 Machine Learning Engineer    RES_10   Candidate_10                 9          0.142380\n",
      "  JD_2 Machine Learning Engineer     RES_5    Candidate_5                 6          0.131336\n",
      "  JD_2 Machine Learning Engineer     RES_9    Candidate_9                 9          0.131336\n",
      "  JD_2 Machine Learning Engineer     RES_4    Candidate_4                 2          0.090440\n",
      "  JD_2 Machine Learning Engineer     RES_8    Candidate_8                 1          0.084433\n",
      "  JD_2 Machine Learning Engineer    RES_11   Candidate_11                 5          0.069631\n",
      "  JD_2 Machine Learning Engineer     RES_7    Candidate_7                 3          0.063316\n",
      "  JD_2 Machine Learning Engineer    RES_12   Candidate_12                 3          0.055518\n",
      "  JD_2 Machine Learning Engineer     RES_3    Candidate_3                 9          0.024017\n",
      "  JD_3              Data Analyst     RES_3    Candidate_3                 9          0.269779\n",
      "  JD_3              Data Analyst    RES_11   Candidate_11                 5          0.267029\n",
      "  JD_3              Data Analyst     RES_7    Candidate_7                 3          0.265593\n",
      "  JD_3              Data Analyst     RES_5    Candidate_5                 6          0.055706\n",
      "  JD_3              Data Analyst     RES_9    Candidate_9                 9          0.055706\n",
      "  JD_3              Data Analyst     RES_1    Candidate_1                10          0.052986\n",
      "  JD_3              Data Analyst    RES_12   Candidate_12                 3          0.045274\n",
      "  JD_3              Data Analyst    RES_10   Candidate_10                 9          0.023942\n",
      "  JD_3              Data Analyst     RES_8    Candidate_8                 1          0.019843\n",
      "  JD_3              Data Analyst     RES_4    Candidate_4                 2          0.019841\n",
      "  JD_3              Data Analyst     RES_2    Candidate_2                 9          0.000000\n",
      "  JD_3              Data Analyst     RES_6    Candidate_6                 5          0.000000\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "üìà KEY STATISTICS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "1. Average match scores by job:\n",
      "\n",
      "   Senior Data Scientist (JD_1):\n",
      "      Average: 0.1460\n",
      "      Best: 0.3039\n",
      "\n",
      "   Machine Learning Engineer (JD_2):\n",
      "      Average: 0.1091\n",
      "      Best: 0.1810\n",
      "\n",
      "   Data Analyst (JD_3):\n",
      "      Average: 0.0896\n",
      "      Best: 0.2698\n",
      "\n",
      "2. Top candidates (highest average across all jobs):\n",
      "\n",
      "   Candidate_1: 0.1727\n",
      "   Candidate_7: 0.1649\n",
      "   Candidate_11: 0.1569\n",
      "   Candidate_5: 0.1557\n",
      "   Candidate_9: 0.1557\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"üìä COMPLETE MATCHING RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print()\n",
    "\n",
    "# Sort by job and score\n",
    "results_df_sorted = results_df.sort_values(\n",
    "    by=['job_id', 'similarity_score'],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(results_df_sorted.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Key statistics\n",
    "print(\"\\nüìà KEY STATISTICS:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(\"\\n1. Average match scores by job:\")\n",
    "for job_id in results_df['job_id'].unique():\n",
    "    job_data = results_df[results_df['job_id'] == job_id]\n",
    "    job_title = job_data['job_title'].iloc[0]\n",
    "    avg_score = job_data['similarity_score'].mean()\n",
    "    max_score = job_data['similarity_score'].max()\n",
    "\n",
    "    print(f\"\\n   {job_title} ({job_id}):\")\n",
    "    print(f\"      Average: {avg_score:.4f}\")\n",
    "    print(f\"      Best: {max_score:.4f}\")\n",
    "\n",
    "print(\"\\n2. Top candidates (highest average across all jobs):\")\n",
    "candidate_avg = results_df.groupby('candidate_name')['similarity_score'].mean().sort_values(ascending=False)\n",
    "print()\n",
    "for candidate, score in candidate_avg.head(5).items():\n",
    "    print(f\"   {candidate}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b73a46",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
